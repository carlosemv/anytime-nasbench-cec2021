%!TEX root = ../emo.tex
\section{Conclusion}
\label{sec:conclusion}

Deep learning breakthroughs in challenging fields such as computer vision have redefined the research in neural architecture search~(NAS~\cite{ReaAggHuaLe2019re,ZopLe2017nas,ZopVasShlLeh2018scalable}). Besides the traditional contributions from neuroevolution, other effective algorithm classes have shown interesting results, such as algorithm configuration~(AC~\cite{ElsMetHut2019nas-survey}). However, the characteristics of deep learning scenarios differ considerably from optimization scenarios for which most effective ACs have been devised. In addition, the computational cost incurred for evaluating architectures currently prohibits extensive ACs runs.

\nasbench is a first effort towards comparability of NAS algorithms. Besides reusable data, authors also provide relevant guidelines for the evaluation and proposal of algorithms. In this work, we have proposed different ways in which the insights produced from this benchmark can be enriched, the most significant being the bi-objective formulation that enables an anytime performance assessment. Not surprisingly, the evaluation of \irace~\cite{LopDubPerStuBir2016irace}, RE~\cite{ReaAggHuaLe2019re}, and SMAC~\cite{HutHooLey2011lion} show that the relative performance of the techniques is not always the same from a final-quality or an anytime performance perspective.

A second contribution from our work is to study the effects of design choices embedded in the original \nasbench assessment. Specifically, we discuss the effects of a variable-sized encoding and caching, and demonstrate that algorithms are affected in different ways by these alternative setups. Finally, we argue against the use of an excessive number of algorithm repetitions, providing evidence that the budget of a configurator could be better spent on more instances or evaluating more architectures.

Besides the future work possibilities discussed along the paper, we highlight two other important pathways. The first is how to account for architecture training time besides accuracy, likely in a bi-objective formulation of architecture evaluation. The second is extending \nasbench to account for more datasets, so effective algorithms devised for multi-instance configuration may identify architectures that are high-performing across different datasets. Altogether, the benefits of these investigations may contribute to reduce dataset-dependent, computationally prohibitive campaigns.