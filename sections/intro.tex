%!TEX root = ../emo.tex
\section{Introduction}

% \leslie{}{
% I write this to clear up ideas. Topics to be covered regarding \nasbench:
% \begin{itemize}
%     \item Number of repetitions of the techniques
%     \item Feasibility regarding execution time of NAS techniques
%     \item Bi-objective formulation (cost/time) or anytime optimization 
%     \item Stochasticity representation missing in the scenario (repetitions) issue in SMAC (caching)
%     \item Representation of the architecture using variable number of nodes.
% \end{itemize}
% }

Neural architecture search~(NAS) is likely the most promising field in automated machine learning~(AutoML) when the data to be modeled is unstructured. Recent NAS breakthroughs include challenging application domains, such as computer vision and natural language processing~\cite{ReaAggHuaLe2019re,ZopLe2017nas,ZopVasShlLeh2018scalable}. These promising applications have evidenced the need for intelligent approaches to mitigate the significant computational effort required for the experimental campaigns involved in the development of NAS techniques.
The development of representative benchmarks that enable fast and standardized experimentation is thus a need in the NAS field.
NAS-Bench-101~\cite{YinKleChrReaMurHut2019nasbench} is a repository of benchmarked convolutional neural networks for the CIFAR-10~\cite{Krizhevsky2009cifar} dataset that can be used by NAS researchers when evaluating or proposing algorithms. \nasbench enables the evaluation of neural architectures in negligible time, providing NAS developers with a fast test suite, and avoiding the duplication of computational effort when evaluating architectures. Besides reusability, \nasbench is also an effort to standardize NAS performance assessment since comparability between different works in the field is in general limited. 


%\leslie{}{[I think here we should introduce why we need to extend the formulation to also include the computational time.]}

In this work, we analyze the design choices in \nasbench and how these relate to the performance insights provided by the benchmark. We propose to enrich the insights provided by \nasbench in three significant ways. First, we formulate NAS as a bi-objective problem comprising solution quality and computational resources consumed. This anytime formulation~\cite{LopStu2013ejor} extends \nasbench final quality approach to an approach that compares the performance dynamics of algorithms in a Pareto-compliant way. Second, we discuss other experimental design choices that underlie \nasbench such as (i)~fixing the number of vertices, which we believe could restrict the potential of NAS algorithms; (ii)~limiting the variability included in the benchmark, which may lead to a weak representation of a NAS scenario, and; (iii)~assessing NAS algorithms solely based on TPU time, overlooking the overhead of the search process of the algorithms. Finally, we revisit the experimental guidelines suggested by \nasbench; specifically, the large number of repetitions suggested to benchmark NAS algorithms. %\carlos{[change paragraph to mirror text/abstract structure?]}

An experimental assessment of high-performing algorithms complements our theoretical discussion. In more detail, we consider two of the best-performing algorithms identified in the original \nasbench assessment, namely regularized evolution~(RE~\cite{ReaAggHuaLe2019re}) and SMAC~\cite{HutHooLey2011lion}. We also include \irace~\cite{LopDubPerStuBir2016irace} in this assessment, an algorithm configurator known to have state-of-the-art performance on the optimization of algorithm design for solution quality~\cite{BezLopStu2020chapter}. Performing a more reasonable number of repetitions than in the extensive campaign adopted in the original \nasbench assessment, we are able to reproduce the same conclusions obtained from the original final-quality performance analysis of RE and SMAC. Furthermore, results show that the algorithm, number of vertices, variability degree, and stopping criterion are interacting factors, confirming our design choice discussion's relevance. 

Even if the algorithms assessed have been configured for final-quality performance, our anytime assessment using a bi-objective optimization methodology enriches the analysis of \nasbench. Specifically, RE is outperformed by the remaining algorithms in the final-quality analysis, but it outperforms \irace regarding anytime performance. We further inspect differences between algorithms with empirical attainment function~(EAF~\cite{LopPaqStu09emaa}) difference plots and observe that RE and SMAC are incomparable under the original \nasbench setup. % Regarding experimental design choices, results show that algorithm, number of vertices, and variability degree are interacting factors, and that SMAC is able to improve significantly under alternative setups proposed. Finally, we remark that our results with a more reasonable number of repetitions match the results from the extensive campaign adopted in the original \nasbench assessment.\sloppy
Regarding the latter, SMAC consistently outperforms the remaining algorithms both concerning final-quality and anytime performance. We believe these results further support our argument that \nasbench assessment should account for NAS computation time, given that the time required by a SMAC execution is considerably larger compared to the time required by a run of RE or \irace.% \carlos{[do we give these times anywhere, or indicate they'll be present in supp material?]}.

We conclude our work with an analysis of the high-performing architectures selected by the NAS algorithms. Interestingly, some of these do not include pooling layers, even if convolutional layers do not constrain feature map dimensions. We believe this result highlights that a bi-objective performance formulation should be discussed not only at NAS level, but also at model training. Though \nasbench provides temporal snapshots of the training process, only the final accuracy of the models is used when assessing an architecture. Thus, costly architectures may be deemed equivalent to much faster ones, clearly an undesirable behavior.

The remainder of this paper is structured as follows. Section~\ref{sec:background} briefly reviews background concepts related to this work, namely neural architecture search, the most relevant NAS algorithm classes and their performance on \nasbench, and anytime optimization. Next, Section~\ref{sec:guidelines} presents our theoretical discussion, in particular the bi-objective formulation for \nasbench and potential impact of benchmark design choices on both final quality and anytime performance. Section~\ref{sec:results} details the final quality results observed for the selected algorithms with regards to the previously discussed benchmark characteristics. The anytime performance of the selected algorithms is discussed in Section~\ref{sec:anytime}. We then conduct an analysis on high-performing architectures in Section~\ref{sec:configurations}. Last, we conclude and discuss future work in Section~\ref{sec:conclusion}.